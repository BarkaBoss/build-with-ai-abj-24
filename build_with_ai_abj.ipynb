{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Welcome to DevFest Abuja 2024\n",
    "<img src=\"img_md/cover.jpg\">\n",
    "\n",
    ">Build with AI is a hands-on event with the goal of launching our community members into AI adoption as solutions to technical and non-technical problems.\n",
    "\n",
    "In this codelab session we will be introducing Google Gemini using python.\n",
    "\n",
    "### What is Google Gemini AI\n",
    ">Google Gemini AI is a multi-modal AI platform that combines text, code, images, and audio to enable developers to build applications with enhanced intelligence.\n",
    "\n",
    "**In summary, it allows developers inject intelligence into their applications**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting things ready for our AI soup\n",
    ">Now that we know what Gemini is, how do we use it?\n",
    "Well... first we need to get the necessary ingredients for our AI soup.\n",
    "\n",
    "### Get your API key\n",
    ">The first thing we need for our AI soup is to get an API key from [Get API Key](https://makersuite.google.com/app/apikey)\n",
    "Create the key and copy the key and keep it safe. **Like all KEYS keep it super safe and private don't share it**\n",
    "\n",
    "\n",
    "<img src=\"img_md/key.png\">\n",
    "\n",
    "\n",
    "This üëÜ is not secure enough by the way\n",
    "\n",
    "Building in Python programming language we need to install the following packages using **pip install**\n",
    "\n",
    "1. **pip install google-generativeai** : facilitates communication between your code and Google Gemini, allows you to query and receive responses from the Gemini API\n",
    "2. **pip install langchain-google-genai** : LangChain is framework designed to allow easy integration of large language models into applications and in this case we are using the package tailored for Gemini.\n",
    "3. **pip install streamlit** : ü§ê\n",
    "\n",
    "### Install them at once\n",
    "pip install google-generativeai langchain-google-genai streamlit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Start your IDEs\n",
    "<img src=\"img_md/engine.png\" width=\"600\">\n",
    "\n",
    "Start you favourite python IDE and get to coding.\n",
    "1. Create a python project or python file \"gem.py\"\n",
    "2. Open the file and enter the following piece of code to import our libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import the necessary libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configure Access to Gemini API"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_API_KEY'] = \"Paste your super secret API here\"\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get a list of Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "for models in genai.list_models():\n",
    "    if 'generateContent' in models.supported_generation_methods:\n",
    "        print(models.name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Select Gemini Pro"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-pro')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Query your model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "================\n",
      "**Ingredients for the Dough:**\n",
      "\n",
      "* 2 cups (240g) all-purpose flour, plus more for dusting\n",
      "* 1/4 cup (50g) granulated sugar\n",
      "* 1 teaspoon active dry yeast\n",
      "* 1/2 teaspoon salt\n",
      "* 1 cup (240ml) warm milk (110-115¬∞F/43-46¬∞C)\n",
      "* 1 large egg, lightly beaten\n",
      "* 2 tablespoons (30g) unsalted butter, softened\n",
      "\n",
      "**Ingredients for the Cream Filling:**\n",
      "\n",
      "* 1 cup (240ml) heavy cream\n",
      "* 1/4 cup (50g) powdered sugar\n",
      "* 1 teaspoon vanilla extract\n",
      "\n",
      "**Instructions for the Dough:**\n",
      "\n",
      "1. In a large bowl, whisk together the flour, sugar, yeast, and salt.\n",
      "2. In a separate bowl, whisk together the warm milk and egg.\n",
      "3. Add the wet ingredients to the dry ingredients and mix until a dough forms.\n",
      "4. Turn the dough out onto a lightly floured surface and knead for 5-7 minutes, or until it becomes smooth and elastic.\n",
      "5. Add the softened butter and knead for an additional minute.\n",
      "6. Place the dough in a lightly oiled bowl, cover with plastic wrap, and let rise in a warm place for 1 hour, or until doubled in size.\n",
      "\n",
      "**Instructions for the Cream Filling:**\n",
      "\n",
      "1. In a medium bowl, whip the heavy cream on high speed until stiff peaks form.\n",
      "2. Gradually add the powdered sugar and vanilla extract while whipping.\n",
      "3. Transfer the cream filling to a piping bag fitted with a star tip.\n",
      "\n",
      "**Instructions for Frying and Filling the Donuts:**\n",
      "\n",
      "1. Heat vegetable oil in a deep fryer or large saucepan to 375¬∞F (190¬∞C).\n",
      "2. Punch down the risen dough and roll it out on a lightly floured surface to a thickness of 1/2 inch (1.25 cm).\n",
      "3. Cut out donuts using a 3-inch (7.5 cm) donut cutter.\n",
      "4. Carefully drop the donuts into the hot oil and fry for 1-2 minutes per side, or until golden brown.\n",
      "5. Remove the donuts from the oil and drain on paper towels.\n",
      "6. While the donuts are still warm, fill them with the cream filling using the piping bag.\n",
      "\n",
      "**Tips for Creamy Donuts:**\n",
      "\n",
      "* Use high-fat dairy products for a richer flavor and creamier texture.\n",
      "* Do not overmix the dough, as this can make the donuts tough.\n",
      "* Fry the donuts at the correct temperature to ensure even cooking and a crispy exterior.\n",
      "* Fill the donuts while they are still warm, as this will help the filling soften and spread.\n"
     ]
    }
   ],
   "source": [
    "print(\"================\")\n",
    "question = input(\"Ask me a question\\n\")\n",
    "print(\"================\")\n",
    "response = model.generate_content(question)\n",
    "\n",
    "print(response.text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Responses Feedbacks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "safety_ratings {\n",
      "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "  probability: NEGLIGIBLE\n",
      "}\n",
      "safety_ratings {\n",
      "  category: HARM_CATEGORY_HATE_SPEECH\n",
      "  probability: NEGLIGIBLE\n",
      "}\n",
      "safety_ratings {\n",
      "  category: HARM_CATEGORY_HARASSMENT\n",
      "  probability: NEGLIGIBLE\n",
      "}\n",
      "safety_ratings {\n",
      "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "  probability: NEGLIGIBLE\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.prompt_feedback)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    ">Response feedbacks are used to manage harmful content from dangerous queries your model might be sent ranking threats from\n",
    "1. HIGH\n",
    "2. MEDIUM\n",
    "3. NEGLIGIBLE\n",
    "3. LOW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting and using Safety Ratings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Osama bin Laden was killed on May 2, 2011, in Abbottabad, Pakistan, by United States Navy SEALs.\n"
     ]
    }
   ],
   "source": [
    "question = input(\"Ask me a question\\n\")\n",
    "print(\"================\")\n",
    "response = model.generate_content(question)\n",
    "\n",
    "print(response.text)\n",
    "feedbacks = response.prompt_feedback.safety_ratings\n",
    "for feedback in feedbacks:\n",
    "    if feedback.category.HARM_CATEGORY_VIOLENCE == feedback.HarmProbability.HIGH or feedback.category.HARM_CATEGORY_VIOLENCE == feedback.HarmProbability.MEDIUM or feedback.category.HARM_CATEGORY_DANGEROUS_CONTENT == feedback.HarmProbability.HIGH or feedback.category.HARM_CATEGORY_DANGEROUS_CONTENT == feedback.HarmProbability.MEDIUM:\n",
    "        print(\"This content is too dangerous\")\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Congrats you have built your first AI with Gemini"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gemini Vision AI Model\n",
    "For image related queries or task we can use the 'gemini-pro-vision' model from the list of gemini models.\n",
    "\n",
    "**create a new python file vision.py and follow along.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = \"Paste your super secret API here\"\n",
    "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now with our packages imported and API key setup we can proceed to creating our model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "vision_model = genai.GenerativeModel('gemini-1.5-flash')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the model created, we can proceed to loading our image using PIL (Python Imaging Library) also known as Pillow\n",
    "Copy/download and image into your project directory\n",
    "\n",
    "Here is my image\n",
    "<img src=\"img_md/download.jpeg\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "image = PIL.Image.open('download.jpeg')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To query our model, we pass the query and the image to model's generate_content([\"Query\", image])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is a professional wrestling match. The wrestler in red and black is Chris Jericho, and the wrestler in yellow and black is Edge. Jericho is performing the Walls of Jericho submission move on Edge.\n"
     ]
    }
   ],
   "source": [
    "response = vision_model.generate_content([\"Explain this image\", image])\n",
    "print(response.text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"img_md/congrats.png\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating your own Chat AI using Gemini\n",
    "\n",
    ">Create a new  python file called chat-gemini.py\n",
    "> Paste the following code in to the file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "st.title(\"GDG Abuja - Gemini Bot\")\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = \"Paste your super secret API here\"\n",
    "genai.configure(api_key = os.environ['GOOGLE_API_KEY'])\n",
    "\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "# Initialize chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = [\n",
    "        {\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\":\"Hello, how can I help?\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Display chat messages from history on app rerun\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Process and store Query and Response\n",
    "def llm_function(query):\n",
    "    response = model.generate_content(query)\n",
    "\n",
    "    # Displaying the Assistant Message\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st.markdown(response.text)\n",
    "\n",
    "    # Storing the User Message\n",
    "    st.session_state.messages.append(\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\": query\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Storing the Assistant Response\n",
    "    st.session_state.messages.append(\n",
    "        {\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\": response.text\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Accept user input\n",
    "query = st.chat_input(\"Hello, how can I help?\")\n",
    "\n",
    "# Calling the Function when Input is Provided\n",
    "if query:\n",
    "    # Displaying the User Message\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(query)\n",
    "\n",
    "    llm_function(query)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the above code using **streamlit run chat-gemini.py**\n",
    "\n",
    "Link to Second Code Lab\n",
    "[Vertex AI CodeLab](https://codelabs.developers.google.com/codelabs/vertex-ai-conversation?hl=en#0)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
